{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install statements from \"Understanding the data\" notebook\n",
    "!pip install neptune-client segmentation_models_pytorch hydra-core kekas -U -q \n",
    "!pip install --target=/kaggle/working pymap3d==2.1.0 -q\n",
    "!pip install --target=/kaggle/working strictyaml -q\n",
    "!pip install --target=/kaggle/working protobuf==3.12.2 -q\n",
    "!pip install --target=/kaggle/working transforms3d -q\n",
    "!pip install --target=/kaggle/working zarr -q\n",
    "!pip install --target=/kaggle/working ptable -q\n",
    "!pip install --no-dependencies --target=/kaggle/working l5kit==1.1.0 --upgrade -q\n",
    "!cp ../input/lyft-config-files/agent_motion_config.yaml config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch-pfn-extras==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                                                                                                  \n",
    "import numpy as np # linear alg\n",
    "import pandas as pd # data processing\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:          \n",
    "        #print(os.path.join(dirname, filename))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://player.vimeo.com/video/389096888', width=640, height=360, frameborder=\"0\", allow=\"autoplay; fullscreen\", allowfullscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U l5kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import l5kit\n",
    "l5kit.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# --- plotly ---\n",
    "from plotly import tools, subplots\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# --- models ---\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "\n",
    "# --- setup ---\n",
    "pd.set_option('max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDataset, AgentDataset\n",
    "\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\n",
    "from l5kit.geometry import transform_points\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from l5kit.data import PERCEPTION_LABELS\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import os\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    filenames.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\"\n",
    "# get config\n",
    "cfg = load_config_data(\"/kaggle/input/lyft-config-files/visualisation_config.yaml\")\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LocalDataManager()\n",
    "dataset_path = dm.require('scenes/train.zarr')\n",
    "zarr_dataset = ChunkedDataset(dataset_path)\n",
    "zarr_dataset.open()\n",
    "print(zarr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = zarr_dataset.frames\n",
    "\n",
    "coords = frames[\"ego_translation\"][:, :2]\n",
    "\n",
    "plt.scatter(coords[:, 0], coords[:, 1], marker='.')\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-2500, 1600])\n",
    "axes.set_ylim([-2500, 1600])\n",
    "plt.title(\"ego_translation of frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_rasterizer = build_rasterizer(cfg, dm)\n",
    "semantic_dataset = EgoDataset(cfg, zarr_dataset, semantic_rasterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectory(dataset, index, title=\"target_positions movement with draw_trajectory\"):\n",
    "    data = dataset[index]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "    draw_trajectory(im, target_positions_pixels, TARGET_POINTS_COLOR, 1,data[\"target_yaws\"])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.imshow(im[::-1])\n",
    "    plt.show()\n",
    "\n",
    "visualize_trajectory(semantic_dataset, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\n",
    "satellite_rasterizer = build_rasterizer(cfg, dm)\n",
    "satellite_dataset = EgoDataset(cfg, zarr_dataset, satellite_rasterizer)\n",
    "\n",
    "visualize_trajectory(satellite_dataset, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(satellite_rasterizer), type(semantic_rasterizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset = AgentDataset(cfg, zarr_dataset, satellite_rasterizer)\n",
    "visualize_trajectory(agent_dataset, index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import PIL\n",
    " \n",
    "dataset = semantic_dataset\n",
    "scene_idx = 34\n",
    "indexes = dataset.get_scene_indices(scene_idx)\n",
    "images = []\n",
    "\n",
    "for idx in indexes:\n",
    "    data = dataset[idx]\n",
    "    im = data[\"image\"].transpose(1, 2, 0)\n",
    "    im = dataset.rasterizer.to_rgb(im)\n",
    "    target_positions_pixels = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n",
    "    draw_trajectory(im, target_positions_pixels,TARGET_POINTS_COLOR, 1, data[\"target_yaws\"])\n",
    "    clear_output(wait=True)\n",
    "    images.append(PIL.Image.fromarray(im[::-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# From https://www.kaggle.com/jpbremer/lyft-scene-visualisations by @jpbremer\n",
    "def animate_solution(images):\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_data(images[i])\n",
    "        return (im,)\n",
    " \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(images[0])\n",
    "    def init():\n",
    "        im.set_data(images[0])\n",
    "        return (im,)\n",
    "    \n",
    "    return animation.FuncAnimation(fig, animate, init_func=init, frames=len(images), interval=60, blit=True)\n",
    "\n",
    "anim = animate_solution(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"agents\", zarr_dataset.tl_faces[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(zarr_dataset.agents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Training with multi-mode confidence\" notebook\n",
    "\n",
    "# --- Dataset utils ---\n",
    "from typing import Callable\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "\n",
    "class TransformDataset(Dataset):\n",
    "    def __init__(self, dataset: Dataset, transform: Callable):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch = self.dataset[index]\n",
    "        return self.transform(batch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function utils\n",
    "# Original code from https://github.com/lyft/l5kit/blob/20ab033c01610d711c3d36e1963ecec86e8b85b6/l5kit/l5kit/evaluation/metrics.py\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_batch(\n",
    "    gt: Tensor, pred: Tensor, confidences: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    Compute a negative log-likelihood for the multi-modal scenario.\n",
    "    log-sum-exp trick is used here to avoid underflow and overflow, For more information about it see:\n",
    "    https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\n",
    "    https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    https://leimao.github.io/blog/LogSumExp/\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(modes)x(time)x(2D coords)\n",
    "        confidences (Tensor): array of shape (bs)x(modes) with a confidence for each mode in each sample\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    assert len(pred.shape) == 4, f\"expected 3D (MxTxC) array for pred, got {pred.shape}\"\n",
    "    batch_size, num_modes, future_len, num_coords = pred.shape\n",
    "\n",
    "    assert gt.shape == (batch_size, future_len, num_coords), f\"expected 2D (Time x Coords) array for gt, got {gt.shape}\"\n",
    "    assert confidences.shape == (batch_size, num_modes), f\"expected 1D (Modes) array for gt, got {confidences.shape}\"\n",
    "    assert torch.allclose(torch.sum(confidences, dim=1), confidences.new_ones((batch_size,))), \"confidences should sum to 1\"\n",
    "    assert avails.shape == (batch_size, future_len), f\"expected 1D (Time) array for gt, got {avails.shape}\"\n",
    "    # assert all data are valid\n",
    "    assert torch.isfinite(pred).all(), \"invalid value found in pred\"\n",
    "    assert torch.isfinite(gt).all(), \"invalid value found in gt\"\n",
    "    assert torch.isfinite(confidences).all(), \"invalid value found in confidences\"\n",
    "    assert torch.isfinite(avails).all(), \"invalid value found in avails\"\n",
    "\n",
    "    # convert to (batch_size, num_modes, future_len, num_coords)\n",
    "    gt = torch.unsqueeze(gt, 1)  # add modes\n",
    "    avails = avails[:, None, :, None]  # add modes and cords\n",
    "\n",
    "    # error (batch_size, num_modes, future_len)\n",
    "    error = torch.sum(((gt - pred) * avails) ** 2, dim=-1)  # reduce coords and use availability\n",
    "\n",
    "    with np.errstate(divide=\"ignore\"):  # when confidence is 0 log goes to -inf, but we're fine with it\n",
    "        # error (batch_size, num_modes)\n",
    "        error = torch.log(confidences) - 0.5 * torch.sum(error, dim=-1)  # reduce time\n",
    "\n",
    "    # use max aggregator on modes for numerical stability\n",
    "    # error (batch_size, num_modes)\n",
    "    max_value, _ = error.max(dim=1, keepdim=True)  # error are negative at this point, so max() gives the minimum one\n",
    "    error = -torch.log(torch.sum(torch.exp(error - max_value), dim=-1, keepdim=True)) - max_value  # reduce modes\n",
    "    # print(\"error\", error)\n",
    "    return torch.mean(error)\n",
    "\n",
    "\n",
    "def pytorch_neg_multi_log_likelihood_single(\n",
    "    gt: Tensor, pred: Tensor, avails: Tensor\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        gt (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        pred (Tensor): array of shape (bs)x(time)x(2D coords)\n",
    "        avails (Tensor): array of shape (bs)x(time) with the availability for each gt timestep\n",
    "    Returns:\n",
    "        Tensor: negative log-likelihood for this example, a single float number\n",
    "    \"\"\"\n",
    "    # pred (bs)x(time)x(2D coords) --> (bs)x(mode=1)x(time)x(2D coords)\n",
    "    # create confidence (bs)x(mode=1)\n",
    "    batch_size, future_len, num_coords = pred.shape\n",
    "    confidences = pred.new_ones((batch_size, 1))\n",
    "    return pytorch_neg_multi_log_likelihood_batch(gt, pred.unsqueeze(1), confidences, avails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model utils ---\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "import pytorch_pfn_extras as ppe\n",
    "from math import ceil\n",
    "from pytorch_pfn_extras.training import IgniteExtensionsManager\n",
    "from pytorch_pfn_extras.training.triggers import MinValueTrigger\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Subset\n",
    "import pytorch_pfn_extras.training.extensions as E\n",
    "\n",
    "class LyftMultiModel(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg: Dict, num_modes=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # To-do: support other models besides resnet18?\n",
    "        backbone = resnet18(pretrained=True, progress=True)\n",
    "        self.backbone = backbone\n",
    "\n",
    "        num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "        num_in_channels = 3 + num_history_channels\n",
    "\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            num_in_channels,\n",
    "            self.backbone.conv1.out_channels,\n",
    "            kernel_size=self.backbone.conv1.kernel_size,\n",
    "            stride=self.backbone.conv1.stride,\n",
    "            padding=self.backbone.conv1.padding,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        backbone_out_features = 512\n",
    "\n",
    "        self.future_len = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "        num_targets = 2 * self.future_len\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            # nn.Dropout(0.2),\n",
    "            nn.Linear(in_features=backbone_out_features, out_features=4096),\n",
    "        )\n",
    "\n",
    "        self.num_preds = num_targets * num_modes\n",
    "        self.num_modes = num_modes\n",
    "\n",
    "        self.logit = nn.Linear(4096, out_features=self.num_preds + num_modes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x = self.backbone.layer2(x)\n",
    "        x = self.backbone.layer3(x)\n",
    "        x = self.backbone.layer4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.head(x)\n",
    "        x = self.logit(x)\n",
    "\n",
    "        # pred (bs) x (modes) x (time) x (2D coords)\n",
    "        # confidences (bs) x (modes)\n",
    "        bs, _ = x.shape\n",
    "        pred, confidences = torch.split(x, self.num_preds, dim=1)\n",
    "        pred = pred.view(bs, self.num_modes, self.future_len, 2)\n",
    "        assert confidences.shape == (bs, self.num_modes)\n",
    "        confidences = torch.softmax(confidences, dim=1)\n",
    "        return pred, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LyftMultiRegressor(nn.Module):\n",
    "    \"\"\"Single mode prediction\"\"\"\n",
    "\n",
    "    def __init__(self, predictor, lossfun=pytorch_neg_multi_log_likelihood_batch):\n",
    "        super().__init__()\n",
    "        self.predictor = predictor\n",
    "        self.lossfun = lossfun\n",
    "\n",
    "    def forward(self, image, targets, target_availabilities):\n",
    "        pred, confidences = self.predictor(image)\n",
    "        loss = self.lossfun(targets, pred, confidences, target_availabilities)\n",
    "        metrics = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"nll\": pytorch_neg_multi_log_likelihood_batch(targets, pred, confidences, target_availabilities).item()\n",
    "        }\n",
    "        ppe.reporting.report(metrics, self)\n",
    "        return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training utils ---\n",
    "from ignite.engine import Engine\n",
    "\n",
    "\n",
    "def create_trainer(model, optimizer, device) -> Engine:\n",
    "    model.to(device)\n",
    "\n",
    "    def update_fn(engine, batch):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss, metrics = model(*[elem.to(device) for elem in batch])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return metrics\n",
    "    trainer = Engine(update_fn)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "def save_yaml(filepath, content, width=120):\n",
    "    with open(filepath, 'w') as f:\n",
    "        yaml.dump(content, f, width=width)\n",
    "\n",
    "\n",
    "def load_yaml(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        content = yaml.safe_load(f)\n",
    "    return content\n",
    "\n",
    "\n",
    "class DotDict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\n",
    "\n",
    "    Refer: https://stackoverflow.com/questions/2352181/how-to-use-a-dot-to-access-members-of-dictionary/23689767#23689767\n",
    "    \"\"\"  # NOQA\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Lyft configs ---\n",
    "cfg = {\n",
    "    'format_version': 4,\n",
    "    'model_params': {\n",
    "        'model_architecture': 'resnet50',\n",
    "        'history_num_frames': 10,\n",
    "        'history_step_size': 1,\n",
    "        'history_delta_time': 0.1,\n",
    "        'future_num_frames': 50,\n",
    "        'future_step_size': 1,\n",
    "        'future_delta_time': 0.1\n",
    "    },\n",
    "\n",
    "    'raster_params': {\n",
    "        'raster_size': [224, 224],\n",
    "        'pixel_size': [0.5, 0.5],\n",
    "        'ego_center': [0.25, 0.5],\n",
    "        'map_type': 'py_semantic',\n",
    "        'satellite_map_key': 'aerial_map/aerial_map.png',\n",
    "        'semantic_map_key': 'semantic_map/semantic_map.pb',\n",
    "        'dataset_meta_key': 'meta.json',\n",
    "        'filter_agents_threshold': 0.5\n",
    "    },\n",
    "\n",
    "    'train_data_loader': {\n",
    "        'key': 'scenes/train.zarr',\n",
    "        'batch_size': 12,\n",
    "        'shuffle': True,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'valid_data_loader': {\n",
    "        'key': 'scenes/validate.zarr',\n",
    "        'batch_size': 32,\n",
    "        'shuffle': False,\n",
    "        'num_workers': 4\n",
    "    },\n",
    "\n",
    "    'train_params': {\n",
    "        'max_num_steps': 10000,\n",
    "        'checkpoint_every_n_steps': 5000,\n",
    "\n",
    "        # 'eval_every_n_steps': -1\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags_dict = {\n",
    "    \"debug\": True,\n",
    "    # --- Data configs ---\n",
    "    \"l5kit_data_folder\": \"/kaggle/input/lyft-motion-prediction-autonomous-vehicles\",\n",
    "    # --- Model configs ---\n",
    "    \"pred_mode\": \"multi\",\n",
    "    # --- Training configs ---\n",
    "    \"device\": \"cuda:0\",\n",
    "    \"out_dir\": \"results/multi_train\",\n",
    "    \"epoch\": 2,\n",
    "    \"snapshot_freq\": 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = DotDict(flags_dict)\n",
    "out_dir = Path(flags.out_dir)\n",
    "os.makedirs(str(out_dir), exist_ok=True)\n",
    "print(f\"flags: {flags_dict}\")\n",
    "save_yaml(out_dir / 'flags.yaml', flags_dict)\n",
    "save_yaml(out_dir / 'cfg.yaml', cfg)\n",
    "debug = flags.debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set env variable for data\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = flags.l5kit_data_folder\n",
    "dm = LocalDataManager(None)\n",
    "\n",
    "print(\"Load dataset...\")\n",
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "valid_cfg = cfg[\"valid_data_loader\"]\n",
    "\n",
    "# Rasterizer\n",
    "rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "# Train dataset/dataloader\n",
    "def transform(batch):\n",
    "    return batch[\"image\"], batch[\"target_positions\"], batch[\"target_availabilities\"]\n",
    "\n",
    "\n",
    "train_path = \"scenes/sample.zarr\" if debug else train_cfg[\"key\"]\n",
    "train_zarr = ChunkedDataset(dm.require(train_path)).open()\n",
    "print(\"train_zarr\", type(train_zarr))\n",
    "train_agent_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "train_dataset = TransformDataset(train_agent_dataset, transform)\n",
    "if debug:\n",
    "    # Only use 1000 dataset for fast check...\n",
    "    train_dataset = Subset(train_dataset, np.arange(1000))\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          shuffle=train_cfg[\"shuffle\"],\n",
    "                          batch_size=train_cfg[\"batch_size\"],\n",
    "                          num_workers=train_cfg[\"num_workers\"])\n",
    "print(train_agent_dataset)\n",
    "\n",
    "valid_path = \"scenes/sample.zarr\" if debug else valid_cfg[\"key\"]\n",
    "valid_zarr = ChunkedDataset(dm.require(valid_path)).open()\n",
    "print(\"valid_zarr\", type(train_zarr))\n",
    "valid_agent_dataset = AgentDataset(cfg, valid_zarr, rasterizer)\n",
    "valid_dataset = TransformDataset(valid_agent_dataset, transform)\n",
    "if debug:\n",
    "    # Only use 100 dataset for fast check...\n",
    "    valid_dataset = Subset(valid_dataset, np.arange(100))\n",
    "else:\n",
    "    # Only use 1000 dataset for fast check...\n",
    "    valid_dataset = Subset(valid_dataset, np.arange(1000))\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    shuffle=valid_cfg[\"shuffle\"],\n",
    "    batch_size=valid_cfg[\"batch_size\"],\n",
    "    num_workers=valid_cfg[\"num_workers\"]\n",
    ")\n",
    "\n",
    "print(valid_agent_dataset)\n",
    "print(\"# AgentDataset train:\", len(train_agent_dataset), \"#valid\", len(valid_agent_dataset))\n",
    "print(\"# ActualDataset train:\", len(train_dataset), \"#valid\", len(valid_dataset))\n",
    "# AgentDataset train: 22496709 #valid 21624612\n",
    "# ActualDataset train: 100 #valid 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(flags.device)\n",
    "\n",
    "if flags.pred_mode == \"multi\":\n",
    "    predictor = LyftMultiModel(cfg)\n",
    "    model = LyftMultiRegressor(predictor)\n",
    "else:\n",
    "    raise ValueError(f\"[ERROR] Unexpected value flags.pred_mode={flags.pred_mode}\")\n",
    "\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train setup\n",
    "trainer = create_trainer(model, optimizer, device)\n",
    "\n",
    "\n",
    "def eval_func(*batch):\n",
    "    loss, metrics = model(*[elem.to(device) for elem in batch])\n",
    "\n",
    "\n",
    "valid_evaluator = E.Evaluator(\n",
    "    valid_loader,\n",
    "    model,\n",
    "    progress_bar=False,\n",
    "    eval_func=eval_func,\n",
    ")\n",
    "\n",
    "log_trigger = (10 if debug else 1000, \"iteration\")\n",
    "log_report = E.LogReport(trigger=log_trigger)\n",
    "\n",
    "\n",
    "extensions = [\n",
    "    log_report,  # Save `log` to file\n",
    "    valid_evaluator,  # Run evaluation for valid dataset in each epoch.\n",
    "    # E.FailOnNonNumber()  # Stop training when nan is detected.\n",
    "    E.ProgressBarNotebook(update_interval=10 if debug else 100),  # Show progress bar during training\n",
    "    E.PrintReportNotebook(),  # Show \"log\" on jupyter notebook  \n",
    "]\n",
    "\n",
    "epoch = flags.epoch\n",
    "\n",
    "models = {\"main\": model}\n",
    "optimizers = {\"main\": optimizer}\n",
    "manager = IgniteExtensionsManager(\n",
    "    trainer,\n",
    "    models,\n",
    "    optimizers,\n",
    "    epoch,\n",
    "    extensions=extensions,\n",
    "    out_dir=str(out_dir),\n",
    ")\n",
    "# Save predictor.pt every epoch\n",
    "manager.extend(E.snapshot_object(predictor, \"predictor.pt\"),\n",
    "               trigger=(flags.snapshot_freq, \"iteration\"))\n",
    "# Check & Save best validation predictor.pt every epoch\n",
    "# manager.extend(E.snapshot_object(predictor, \"best_predictor.pt\"),\n",
    "#                trigger=MinValueTrigger(\"validation/main/nll\", trigger=(flags.snapshot_freq, \"iteration\")))\n",
    "# lr scheduler\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#     optimizer, mode='min', factor=0.7, patience=5, min_lr=1e-10)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer, gamma=0.99999)\n",
    "manager.extend(lambda manager: scheduler.step(), trigger=(1, \"iteration\"))\n",
    "# Show \"lr\" column in log\n",
    "manager.extend(E.observe_lr(optimizer=optimizer), trigger=log_trigger)\n",
    "\n",
    "# *hacking* to fix ProgressBarNotebook bug\n",
    "manager.iteration = 0\n",
    "manager._iters_per_epoch = len(train_loader)\n",
    "\n",
    "trainer.run(train_loader, max_epochs=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = log_report.to_dataframe()\n",
    "df.to_csv(out_dir/\"log.csv\", index=False)\n",
    "df[[\"epoch\", \"iteration\", \"main/loss\", \"main/nll\", \"validation/main/loss\", \"validation/main/nll\", \"lr\", \"elapsed_time\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls results/multi_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
